%
\section{Testing}
%
thesis.fmitcar.appspot.com/thesis/start/
%
\subsection{Test Environment}
%

%
All our video summaries are hosted on YouTube.
%
\subsubsection{Questionnaire}
%
% reverse engineer questions (find lit. on how to write a questionnaire)
%
The questionnaire was structured as a statement and 5 options
%
% Noter fra Kim: Vis figur med spørgsmål
%
where the test user could either agree, disagree, or indicate that they were unable to respond to the statement. An example of a statement was INSERT STATEMENT HERE and the possible replies was Totally agree, Somewhat agree, Somewhat disagree, Totally disagree, and Don't know, in that order. We initially had "dont't know" in the middle, but we learned that placing it last would result in a higher rate of usable responses.\\
%
% Noter fra Kim: Har i så smidt resultater væk?
%
% Noter fra Lauge: Bør vi nok fjerne. Ikke super relevant. 
%
%
We want to investigate a few crucial points in depth which resulted in 4 very different statements:
%
\begin{itemize}
\item The individual video clips are interesting
\item The video is well edited
\item The length of the individual video clips are fitting
\item The length of the entire video is fitting
\end{itemize}
%
The first statement investigates the quality of our labeller. The second statement investigates how well we are able to combine segments (in other words the quality of our recipe, ie. the order and context of the ingredients). The third statement investigates the general attitude towards clip-length, which is chosen somewhat at random within boundaries determined by the recipe. The fourth statement investigates the general attitude towards the total length of a video-summary. The ordering of statements changes between asking to the enterity of the video to each individual clip.\\
We also added an optional note to each questionnaire in case a user wanted to ellaborate on their answer(s).
%
\subsubsection{Setup}
%
As we both have a good deal of experience with \textit{Google App Engine} we choose to deploy a small website where our test panel could rate video-summaries. We used a spartan styling of the site such as a white background.
%
% Noter fra Kim: What?
%
% Noter fra Lauge: Minimalistic? 
%
%
To discern interuptive elements we removed player-controls from the YouTube embedded player (the user could still pause the video by clicking on it or using the space-bar on their keyboard), and also disable the recommended videos that appear after playing a YouTube video to the end. The questionnaire was not shown before the end of the video in order not to disctract attention to it. The video-title, which would appear briefly in the top of the player was the hex-value of a randomly generated number to ensure a non-descript title.\\
We supplied each session with a unique identifier. This means that we are unable to track a returning test-user, as they would get a new session id, but it does give us a rough measure of how many different test-users we get, how many questionnaires answered on average, and also investigate the progress of a test-user (they may generally rate a video worse as they watch more videos) - the time at which a result is recorded is saved along with the data.\\
To increase the availability of our test environment everything is localized into danish and english - especially elderly danish people may have a hard time answering an english questionnaire.
%
% Noter fra Kim: Hvad er de danske oversættelser? De skal angives
%
The resulting locale is recorded along with each result. This will also allow us to measure any significant difference between the two locales - as it is impossible to make a 1-to-1 translation of the statements.\\
To ensure a proper shuffling of each video-summary we record how many times a given summary is shown (it counts as shown when a test-user has completed the questionnaire). We consequently enforce a balance of this counter amongst all the videoes. Furthermore we restrict, if possible, that a video has not been shown recently (within the last 3 minutes). This would allow us to easily add more videos later on should the need arise, while getting results from the entire range of summaries.\\
%
% Noter fra Kim: Det forstår jeg ikke
%
% Noter fra Lauge: Det er heller ikke helt tilfældet som vi også sidenhen snakkede om.
%
%
We initially spread the survey on Facebook by messaging friends and family and making a public wall post. To further increase the chance of the survey going viral we placed a Facebook Like-button and a Twitter button discretely in the bottom of the site to allow test-users to further spread the word.\\
We designed the test so that the test-user can watch as many or as few videos as they want, ie. each video-summary is a self-contained questionnaire.
%
\subsubsection{Control Samples}
%
We also added two different control samples: one was randomly generated videos, but from a random set of labels and segment lengths, but also some where sections of a video was picked totally at random.
%
% Noter fra Kim: See section XX
%
This will form the baseline for our test, ie. we expect these to perform poorly in all aspects. We also added the opposite, videos edited by humans as another control sample. We expect these to perform better than all other videos.
%
\subsection{Results}
%

%
\subsection{Analysis}
%

%
\subsubsection{Risks \& Limitations}
%


%
\subsection{Conclusion}
%