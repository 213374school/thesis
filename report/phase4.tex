%
\section{Testing}
%
test site can be found at thesis.fmitcar.appspot.com/thesis/start/
%
\subsection{Test Environment}
%
Our test environment consists of a website showing a collection of generated videos, hosted on YouTube. Users see one video on each page and are presented with a questionnaraire after the video is done playing.
%
\subsubsection{Questionnaire}
%
% reverse engineer questions (find lit. on how to write a questionnaire)
%
The questionnaire for each video consists of four statements to which the user can \textit{disagree}, \textit{somewhat disagree}, \textit{somewhat agree}, \textit{agree} or declare that she \textit{does not know}. In an attempt to discourage the user from selecting the latter option it is shown last in the list. Based on feedback from an initial, single-user test run we also added and introduction page that explains the purpose of the test and shows the available statements and options. The description is also present throughout the test.\\
%
The four statements shown after each video are:
%
\begin{itemize}
\item The individual video clips are interesting
\item The video is well edited
\item The length of the individual video clips are fitting
\item The length of the entire video is fitting
\end{itemize}
%
The first statement investigates the quality of our labeller. The second statement investigates how well we are able to combine segments (in other words the quality of our recipe, ie. the order and context of the ingredients). The third statement investigates the general attitude towards clip-length, which is chosen somewhat at random within boundaries determined by the recipe. The fourth statement investigates the general attitude towards the total length of a video-summary. We also added an optional note to each questionnaire in case a user wanted to ellaborate on their answer(s).
%
\subsubsection{Setup}
%
Our test environment runs on \textit{Google App Engine} and consist of a minimalistic website. To discern interuptive elementsplayer-controls are removed from the YouTube embedded player (the user can still pause the video by clicking on it or by using the space-bar on their keyboard). The questionnaire was not shown before the end of the video in order not to disctract attention to it. The video-title, which would appear briefly in the top of the player is the hex-value of a randomly generated number to ensure a non-descript title.\\
Each user session is tracked using a randomly generated identifier. We do not make any attempt to track users across different sessions so we are unable to track a returning test-user, but it does give us a rough measure of how many different test-users we have, how many questionnaires they answered on average.\\
To increase the availability of our test environment everything is localised into danish and english. The resulting locale is recorded along with each result, in case the specific translations have an effect on our results. All text in the test is available in Appendix XX, in both languages.\\
The test is designed so that the test user can see as few or as many videos as she wants. A circular queue of videos is shared between all users, meaning that a new user will start at the video following the one that was last rated. Answers are stored after each succesful rating.\\
The test users concist of friends and relatives contacted through facebook. In order to help spread the test to more people we also placed a Facebook Like-button and a Twitter button discretely in the bottom of the questionnaires.\\
%
\subsubsection{Control Samples}
%
%
\subsubsection{Random Segments}
% 
For each dataset we created a videoclip totally at random, ie. we picked a random video, and a random segment within that video, and cut it all together. The segment length was chosen such that they would not differ significanytly from the general length of other computer generated videoclips. These videoclips are thought of as a baseline (we expect them to perform poorly compared to even random labels recipe).
%
\subsubsection{Human Edited}
% 
We used humanly edited videoclips as a control sample. A select few videos was cut to fit the general length of other computer generated videoclips.
%
% Noter fra Kim: Hvor kommer de fra?
%
We add two different control samples: one is videos with a random set of labels and segment lengths, and one where clips was picked totally at random.
%
% Noter fra Kim: See section XX WOOOOT??? ANDERS
%
This will form the baseline for our test, ie. we expect these to perform poorly in all aspects. We also added videos edited by humans as a control sample. We expect these to perform as well as the designer recipies or better.
%
\subsection{Results}
%
The first step in our analysis of the collected data is to plot histograms of the answers in each method, as seen in Figure \ref{fig:hist_design} (additional figures can be found in Appendix \ref{app:histograms}), in order to investigate the distribution of answers. As see in Figure \ref{fig:hist_design} the answers are unevenly distributed with a skew to the right with the exception of the first histogram (content). The answers are generally unevenly distributed in the other histograms which allows us to continue our analysis. Had there been a trend of an even distribution of answers the data would have been inconclusive by definition.
%
\begin{figure}
     \centering
     \includegraphics[width=1.0\textwidth]{img/designer_barplot.png}
     \caption{Histogram of answers to videos created with the "designer method"}\label{fig:hist_design}
\end{figure}
%
We continue our analysis by employing the Friedman test\footnote{http://en.wikipedia.org/wiki/Friedman\_test} which is a non-parametric statistical test (we make no assumptions on the normality of our data). The results can be found in Tables \ref{tab:fried_alpha}, \ref{tab:fried_dataset}, and \ref{tab:fried_recip}. 
%
\input{matrix2latex/recipies}
\input{matrix2latex/datasets}
\input{matrix2latex/alpha-span}
%
present notes + v = DoF, p-value, $x^2$ . hvad er i tables % ANDERS
%
\subsection{Analysis}
%
The only statistical significance is content in Table \ref{tab:fried_dataset}, which is no surprise as the datasets differ primarily on content as described in section \ref{section that describes datasets}. As everything else is statistically insignificant we do not pursue further analysis KIM: VI SKAL LIGE SNAKKE OM HVAD VI GØR HER.
%
\subsubsection{Risks \& Limitations}
%
User based tests are inherently risky. Our choice of statements and options are bound to have had a huge influence on the results. As is the choice of supporting two languages. Based on feedback from users (both through the test and in person) we have come to understand that several things could have been done better. Several users complained over the lack of context in which they were watching the video. They claimed, rightfully, that it is almost impossible to determine whether clips were interresting or whether the length of a video is fitting, when they did not know what they were watching. The lack of sound only makes this more difficult.\\\\
%
Lots more here...
% LAUGE
% limited dataset -> we did not have seperate training and test sets\\     <- Skal stå i forrige sektion
% barely acceptable amount of feedback -> could have caused ...
% search keywrod kunne måske have hjulpet på at forstå spøgsmål bedre.
%
\subsection{Conclusion}
%
% ANDERS
%