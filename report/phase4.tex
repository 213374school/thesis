%
\section{Findings}
%
Due to the subjective nature of estimating the overall quality of our video summaries we test our results on a panel of test users. We use various methods to generate the summaries and present them to the panel for review.
%
\subsection{Test Environment}
%
Our test environment is a website showing a collection of generated videos, hosted on YouTube. Visitors are shown one video at a time, and are presented with a questionnaire after the video is done playing.
%
\subsubsection{Questionnaire}\label{sec:questionnaire}
%
% TODO: reverse engineer questions (find lit. on how to write a questionnaire) - may shoot ourselves in the foot :D
%
The questionnaire for each video consists of four statements to which the user can \textit{disagree}, \textit{somewhat disagree}, \textit{somewhat agree}, \textit{agree} or declare that she \textit{does not know}. In an attempt to discourage the user from selecting the latter option it is shown last. Based on feedback from an initial, single-user test run we also added and introduction page that explains the purpose of the test and shows the available statements and options. The description is also present throughout the test. Part of the website is shown in Figure \ref{fig:questdump}.\\
%
The four statements shown after a video is:
%
\begin{itemize}
\item The individual video clips are interesting
\item The video is well edited
\item The length of the individual video clips are fitting
\item The length of the entire video is fitting
\end{itemize}
%
The first statement investigates the quality of our choice of labels, and to some degree also the quality of our classifiers. The second statement investigates how well we are able to combine segments (in other words the quality of our recipe, ie. the order and context of the ingredients). The third statement investigates the general attitude towards clip-length, which is chosen somewhat at random within boundaries determined by the recipe. The fourth statement investigates the general attitude towards the total length of a video-summary. We also added an optional note to each questionnaire in case a user wanted to ellaborate on her answer(s).
%
\begin{figure}
     \centering
     \includegraphics[width=1.0\textwidth]{img/quest_dump.png}
     \caption{Example of questionnaire on test-site}\label{fig:questdump}
\end{figure}
%
\subsubsection{Setup}
%
Our test environment runs on \textit{Google App Engine} and consist of a minimalistic website. To discern interuptive elementsplayer-controls are removed from the YouTube embedded player (the user can still pause the video by clicking on it or by using the space-bar on their keyboard). The questionnaire was not shown before the end of the video in order not to disctract attention to it. The video-title, which would appear briefly in the top of the player is the hex-value of a randomly generated number to ensure a non-descript title.\\
Each user session is tracked using a randomly generated identifier. We do not make any attempt to track users across different sessions so we are unable to track a returning test-user, but it does give us a rough measure of how many different test-users we have, how many questionnaires they answered on average.\\
To increase the availability of our test environment everything is localised into danish and english. The resulting locale is recorded along with each result, in case the specific translations have an effect on our results. All text in the test is available in Appendix [SEC:REF], in both languages.\\
The test is designed so that the test user can see as few or as many videos as she wants. A circular queue of videos is shared between all users, meaning that a new user will start at the video following the one that was last rated. Answers are stored after each succesful rating.\\
The test users concist of friends and relatives contacted through facebook. In order to help spread the test to more people we also placed a Facebook Like-button and a Twitter button discretely in the bottom of the questionnaires.\\\\
%
The test-site can be found at: http://thesis.fmitcar.appspot.com/thesis/start/ and a screendump of this is shown in Figure \ref{fig:website}
%
\begin{figure}
     \centering
     \includegraphics[width=1.0\textwidth]{img/website.png}
     \caption{Example of test-site}\label{fig:website}
\end{figure}
%
\subsection{Test videos}
%
We investigate several aspects of the video summary aggregation. Most importantly we investigate how our automatically generated video summaries compare to human edited ones, and how both these groups compare to randomly generated video summaries.
% KIM: generalt noget om hvilke processer/faser som har indvirkning på de følgende videoer. hvad har vi med/ikke med og hvorfor? evt. noget om indvirkningen på videoen (hvad vi vurderer)
\subsubsection{Random}
%
\textit{Random} video summaries are generated without any use of labels. They consist of 6 videoclips selected randomly from the footage for a particular event. Each clip has a random length between 2 and 10 seconds. Only one clip is selected from each video, in order to avoid repeating or overlapping footage, which would be a dead giveaway. These video summaries are thought of as a baseline - we expect them to perform poorly.
%
\subsubsection{Random label}
%
These video summaries are slightly \textit{less} random. Instead of selecting video clips randomly from the footage, we instead generate a random recipe and create a video summary from it. This is done by generating a sequence of 5 \textit{ingredients} (described in section [SEC:REF]), each containing a random set of 1 to 3 \textit{requested labels}. The minimum length of a video clip, $min$, is randomly set to be between 2 and 9 seconds. The maximum length is randomly set to be between $min$ and $2 \cdot min$.\\
Because these video summaries are based on footage where labels were detected, we expect their content to be of higher quality than the videos described in the previous section.
%
\subsubsection{Designer}\label{sec:phase4designer}
%
The \textit{designer} video summaries are based on our own recipe. The ingredients are outlined below:
% KIM: skriv meget mere om hvad er hvad. altså hvad er overview shot, crowd shot, etc. hvad betyder x-y seconds? etc.
\begin{enumerate}
% KIM: ingredients must come in this order?
\item Overview shot, 3-5 seconds, no person in focus
\item Overview shot, 3-6 seconds, no person in focus
\item Crowd shot, 3-6 seconds, no person in focus
\item Crowd shot, 3-6 seconds, no person in focus
\item Person in focus shot, 4-8 seconds, not in a crowd
\item Person in focus shot, 4-8 seconds, not in a crowd
\item Overview shot, 3-5 seconds, no person in focus
\end{enumerate}
%
During recipe tuning we realized that the overview label and person in focus label often overlapped. Although the two labels are mutually exclusive in a single 
% KIM: frame? segment?
% ANDERS: jeg fatter heller ikke en hvede af det her? hvad prøver vi at sige?
, a segment containing both can still be generated if its score is sufficiently big (see section [SEC:REF til Segment Score]). To reduce the odds of having a person in focus on overview shots we put this label into the forbidden labels list (an overview can still have a person in focus not caught by our classifiers).\\
Likewise there are typically two types of crowd shots. One, which has someone in focus, and the other one, which has not. We wanted the latter type.
%
\subsubsection{Human edited}
%
We used human edited video summaries as a control sample. These videos are taken from some of the orginal videos, which we found on YouTube and later split up. Therefore they contain the same footage as is available to the \textit{random-}, \textit{random label-}, and \textit{designer-} summarizers. This should ensure that they do not look too different. We expect these to perform as well as the designer recipies or better.
%
\subsubsection{Other aspects}\label{sec:otheraspects}
%
% KIM: hvorfor kun teste a=0.25 og a=0.50? og hvad svarer værdien til? argumenter for valget.
On top of comparing the quality of the different types of videos described above, we also investigate the result of using different $\alpha$-span values to regulate the weighting between label- and timespan- fulfilment. Different values should affect both the length of the individual video clips, and the total length of the video summary.\\
Finally, we also want to know if the actual events in the footage has any influence of our results.\\\\
%
In total there are 21 videos summaries in the test. Their distribution is shown in figure \ref{tab:videosintest}.
%
% KIM: inkluder titel og beskriv hvordan man kan se video
\begin{table}\centering
    \begin{tabular}{|l|l|l|}
        \hline
        Method       & Event           & $\alpha$-span \\ \hline
        random       & ACTA Aarhus     & ~           \\ 
        random       & ACTA Copenhagen & ~           \\ 
        random       & COP 15          & ~           \\ 
        random label & ACTA Aarhus     & 0.25        \\ 
        random label & ACTA Aarhus     & 0.50        \\ 
        random label & ACTA Copenhagen & 0.25        \\ 
        random label & ACTA Copenhagen & 0.50        \\ 
        random label & COP 15          & 0.25        \\ 
        random label & COP 15          & 0.50        \\ 
        designer     & ACTA Aarhus     & 0.25        \\ 
        designer     & ACTA Aarhus     & 0.50        \\ 
        designer     & ACTA Copenhagen & 0.25        \\ 
        designer     & ACTA Copenhagen & 0.50        \\ 
        designer     & COP 15          & 0.25        \\ 
        designer     & COP 15          & 0.50        \\ 
        human edited & ACTA Copenhagen & ~           \\ 
        human edited & ACTA Copenhagen & ~           \\ 
        human edited & ACTA Copenhagen & ~           \\ 
        human edited & ACTA Copenhagen & ~           \\ 
        human edited & COP 15          & ~           \\ 
        human edited & COP 15          & ~           \\
        \hline
    \end{tabular}
\caption{Distribution of videos in the test}\label{tab:videosintest}
\end{table}
%
\subsection{Results}
%
We start by looking at an overview of the data we gathered, and then go into detail with the specific results.
%
\subsubsection{Overview over the feedback}
%
At the time of writing, our test has been online for around 4 weeks. In that time 160 videos have been observed and rated over 25 sessions. HVAD ER EN SESSION?\\
The average number of answers per session is 6.4, with a median of 5.0. The shortest session contains 1 answer and the longest contain 25\\
In addition to the answers required for each video, we also received 11 notes from the test users. REFERENCE TIL HVOR DE KAN SES (HAR LAVET TABEL)\\
108 (67.5\%) are answers from the danish version of the test. 52 (32.5\%) is from the enlish version. Each videos in the test is rated 7 or 8 times.
%
\subsubsection{Histograms}\label{sec:histograms}
%
Histograms shows the distribution of data, and is a visual tool to aid in determining if data follows a specific probability distribution. There are 5 bins in the histograms, where each bin corresponds to a specific answer to our questionnaire. Each bin is thus, from left to right, Totally Disagree (TD), Somewhat Disagree (SD), Don't Know (DK), Somewhat Agree (SA), Totally Agree (TA). The answers are divided into groups based on how they where created, as described in section \ref{sec:otheraspects}.
%
\begin{figure}[!ht]
     \centering
     \includegraphics[width=0.7\textwidth]{img/totallyrandom_barplot.png}
     \caption{Histogram of answers to videos created using random segments}\label{fig:hist_random}
\end{figure}
%
\begin{figure}[!ht]
     \centering
     \includegraphics[width=0.7\textwidth]{img/labelrandom_barplot.png}
     \caption{Histogram of answers to videos created using the random label recipe}\label{fig:hist_labelrandom}
\end{figure}
%
\begin{figure}[!ht]
     \centering
     \includegraphics[width=0.7\textwidth]{img/designer_barplot.png}
     \caption{Histogram of answers to videos created using the designer method}\label{fig:hist_design}
\end{figure}
%
\begin{figure}[!ht]
     \centering
     \includegraphics[width=0.7\textwidth]{img/humanedited_barplot.png}
     \caption{Histogram of answers to videos that are edited by a human}\label{fig:hist_human}
\end{figure}
%
\subsubsection{Friedman Test}\label{sec:friedman}
%
The Friedman rank sum test
% KIM: reference
 is a non-parametric statistical test, and is used to detect differences in methods in multiple tests. In our case we have multiple methods, datasets, and parameters, and each test participant can be viewed as a seperate test.\\
 We test against the null-hypotheses stated as follows: 
 %which in a general form is that there is no significant measureable difference in the methods applied to generate a video. We test for differences in recipies, datasets and $\alpha-$span values, where each null-hypothesis is as follows:
 \begin{itemize}
 \item There is no significant measureable difference in the methods used to generate a video
 \item There is no significant measureable difference in the dataset used to generate a video
 \item There is no significant measureable difference in the $\alpha-$span parameter used to generate a video
 \end{itemize}
%
The histograms in section \ref{sec:histograms} shows us that the data does not follow any specific distribution, which forces us to deploy a non-parametric statistical test. The disadvantage of employing a non-parametric test is that it requires a larger sample size to draw conclusions with the same degree of confidence as a parametric test.\\
%
The values related to each answer are also not compareable (there is no clear numerical interpretation), ie. \textit{Totally Disagree} $(-2)$ is not twice as bad as \textit{Somewhat Disagree} $(-1)$, but \textit{Totally disagree} is ranked lower than \textit{Somewhat disagree}. The Friedman test ranks all answers, where answers with equal values receive the same rank, and then sums occurences of each rank.\\
%
Results of our Friedman test can be found in Tables \ref{tab:fried_alpha}, \ref{tab:fried_dataset}, and \ref{tab:fried_recip} where $\nu$ denotes degrees of freedom, and p-value is \textit{"the probability of obtaining a test statistic at least as extreme as the one that was actually observed, assuming that the null hypothesis is true."}\footnote{http://en.wikipedia.org/wiki/P-value}. A p-value less than $0.05$ is considered statistically significant.
% INPUT IS 3 TABLES
\input{matrix2latex/recipies}
\input{matrix2latex/datasets}
\input{matrix2latex/alpha-span}
%
\\ %newline after input table
Table \ref{tab:fried_recip} shows the result from videos generated by the each method, Table \ref{tab:fried_dataset} shows the results from videos partitioned by which dataset a given video is generated from, and Table \ref{tab:fried_alpha} shows the effect of different $\alpha$-span values (described in section \ref{sec:recipe})
%
\subsubsection{Comments}\label{sec:comments}
%
Table \ref{tab:notes} show some of the comments we recieved on each video. We have omitted the specific video in question, but have shown the recipe the video was created with and also the dataset the video is based on. The comments are shown unedited in their original language.
%
\input{matrix2latex/notes}
%
\subsection{Analysis}
%
Figure \ref{fig:hist_random} (Random) and \ref{fig:hist_human} (Human edited) shows that answers are unevenly distributed with a skew to the right, ie. test participants generally agrees with the statements. Figure \ref{fig:hist_labelrandom} (Random label) shows that answers are somewhat evenly distributed, not counting the Don't Know answer. Figure \ref{fig:hist_design} (Designer) shows that answers are unevenly distributed with a skew to the right with the exception of the "content" statement.\\
We are unable to reject the null hypothesis in almost all regards and conclude that the questionaires reveal no significant effect on the various methods of generating a video. This also means that there is no significant measureable difference between a human edited video and one that is generated at random.\\
%
There is also no significant measureable effect of choosing different $\alpha$-values when generating videos. This means that we are unable to discern if longer or shorter scenes in a video is desired.\\
%
There was also no significant measureable difference between the three datasets, except in content, which is no surprise as the datasets differ primarily on content as described in section \ref{sec:dataset}.\\
%
We have omitted analysis of the comments in Table \ref{tab:notes} and instead included them in the discussion section of this chapter.\\
%
% KIM: god ide. gogo på analyse
FURTHER ANALYSIS COULD BE TO INVESTIGATE THE EFFECT OF ALPHA VALUES, IE. FOR ALPHA VALUE X WE GET A TOTAL AVR. VIDEO LENGTH OF SO AND SO SECONDS, AND SEGMENT LENGTHS OF SO AND SO SECONDS, COMPARE THAT TO AN ALPHA VALUE OF 2X.\\
%
\subsection{Risks \& Limitations}
%
% KIM: dette skal flettes ind under diskussionsafsnittet. gerne som subsection
User based tests are inherently risky. Our choice of statements and options are bound to have a huge influence on the results. As is the choice of supporting two languages.\\
The size of our dataset is also a problem. We only have a little bit more than an hour of usable footage. Of this, only a part is actual raw footage. The rest is from pre-edited videos, which we later split up into individual scenes and use as raw footage. This is bound to increase the overall quality, which is especially a problem with the \textit{random} and \textit{random label} videos we generate, which are suppose to function as baselines in the test.\\
% KIM: omformuler!
- Number of test users / answers
\\\\
%
Lots more here...
% LAUGE
% limited dataset -> we did not have seperate training and test sets\\     <- Skal stå i forrige sektion
% barely acceptable amount of feedback -> could have caused ...
% search keywrod kunne måske have hjulpet på at forstå spøgsmål bedre.
%
\subsection{Discussion}
%
The histograms showed us that test participants are generally in agreement with the statements proposed to them. We believe that this could be a result of the participants being friends and family of the authors, as they may be inclined to believe that giving a positive answer will help us more than being perfectly honest. If we adjusted for this effect the histograms would likely have been much more evenly distributed.\\
%
The test revealed no significant measureable effect in any regard, and we believe that one of the causes is high quality raw material. We suspect that there would have been a noticeable difference if our dataset had contained a larger amount of video footage of questionable quality; both visual and contextual. This does however primarily account for the questionnaire statement regarding content. The other three statements are more related to other aspects of video generation.\\
The lack of significant measureable effect can also be a result of the questionnaire, ie. the wording of each statement (and answer to) may not give us the information we are looking for, as described in section \ref{sec:questionnaire}. A non-parametric statistical test requires a larger sample size to draw conclusions with the same degree of confidence as a parametric test as mentioned in section \ref{sec:friedman}, and we did only get between 7 and 8 answers for each video. In that regard we also had to limit ourselves in how many videos we could produce for each category, as seen on Table \ref{tab:videosintest}.\\
Based on feedback from users (both through the test and in person) we have come to understand that several things could have been done better. Several users complained over the lack of context in which they were watching the video. They claimed, rightfully, that it is almost impossible to determine whether clips were interresting or whether the length of a video is fitting, when they did not know what they were watching. The lack of sound only makes this more difficult. One suggestion for improvement, we received, was to preceed each statement with a written context describing what was being shown.\\
TODO:\\
DISCUSSION ON COMMENTS\\
DISCUSSION ON STEPS IN PIPELINE, IE. HOW DID STEP 0 AFFECT STEP 1, AND STEP 1 AFFECT STEP 2, ETC. - BELONGS IN DISCUSSION FOR ENTIRE REPORT!
%
\section{Summary}
%
We have described a way to automatically generate video summaries based on a defined \textit{recipe}. We have shown how, given a collection of labelled footage, we can estimate an overall fulfilment ratio for each segment of said footage, and use this an indicator of what to include in the final video.\\
We have compared this method with various control groups, both consisting of randomly generated-, but also of humanly edited- videos. This was done through an online survey were a group of test users watched the different videos and rated them on various aspects.\\
The results from this survey are inconclusive. The test users ability to differentiate between the various types of videos is not statistical significant. We have speculated on the various possible reasons for these findings.\\
% KIM: og kommet frem til?
