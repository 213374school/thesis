\chapter{Discussion}
%
% labeling was so dependant on image quality that the quality assesment was not as much needed (other than optimizations we did not perform)
% impact of generally high quality videos (pre-edited and then cut up)
% gold standard is somewhat error prone
%
\chapter{Conclusions}
%
% \section{Summary}\label{sec:conc_summary}
%
Brief summary of the thesis
%
\section{Conclusions}\label{sec:conc_concs}
%
% Explain the current state-of-the-art on the topic of automatic construction of videos 
% -> Uncover the current state-of-the-art on the topic of automatic video summaries from multiple sources
Our first research goal was to uncover the current state-of-the-art on the topic of automatic video summaries from multiple sources. Girgensohn et al.\cite{Girgensohn:2000:SAH:354401.354415} describes a method for a semi-automatic way of editing home video, in which they use automated video analysis to identify suitable clips. However they do not perform any contextual analysis of the footage.
\\\\
% Select, possibly extend, and implement a method for segmenting video clips.
Our image quality assessment was based on methods presented by Girgensohn et al.\cite{Girgensohn:2000:SAH:354401.354415} who proposes a simple and effective way of detecting camera movement frame-by-frame by computing a vector describing the direction and magnutide of the movement. They also proposed a method of detecting overly dark images, on which we improved to also detect overly bright images, or images with little texture such as extreme closeups.\\
We proceeded with different approaches to classifying a single frame of video footage and compare the classification to a gold standard. Several of our approaches provided satisfactory results, but we do question the way we achived the gold standard as this included manual viewing of the entire dataset by one or more human observers and recording their subjective X. OMFORM!\\
% Develop and implement a method for labeling video segments.
We successfully labeled video footage based on a contextual analysis. The analysis was based on metadata from the image quality assesment, and metadata computed for the purpose of labelling. We were able to identify public speakers, footage shot from within a crowd, detect police blinker lights, and overview shots. 
\\
The correctness of each type of label varies, but the \emph{Person in Focus}-label was correct $X\%$ of the time. Other labels such as the \emph{Vertical Oscillation} bla bla. skriv mere om dette. SKAL BRUGE DISSE TAL!
\\\\
% Develop and implement a method for composing a coherent video summary by aggregating video segments. 
% -> Develop a method for composing a video summary from multiple sources with a coherent contextual flow
When we had completed labelling the video footage we were ready to create a video summary. We used a cooking analogy where a recipe would be a list of ingredients in a specific order. For each ingredient we try to find the most fitting sub-part of a video that matches the description of the ingredient, where the ingredient would be a list of parameters such as labels required and how long the part should be. For this purpose we designed an algorithm that computes a score for each sub-part based on an input ingredient and available footage.\\
We initially had plans of using machine learning for creating the summary, but this would require extensive training, possibly involving external feedback, and simply would not fit within the scope of the project.
\\\\
% Evaluate the final architecture and prototype with respect to domain-specific metrics (primarily focusing on quality and processing time)
The inspiration to the entire project was an idea of a service that would process live video-streaming into a live video-summary. Such a service impose a low treshold on processing time for each video-stream. Image quality assessment is achiveable during playback and provides a good base for later optimization as we experienced that contextual analysis is mostly fruitless when performed on video footage of questionably quality. Ie. contextual analysis is only strictly needed on high quality footage, and an efficient approach as such. Object detection was one of the most ressource consuming processes, although capable of a live-analysis, we attempted to detect multiple objects (face, profile, and upper body), making multi-processing an absolute must. Most research the area of object detection in images is focused on still images, but research does exist on ex. detecting cars in traffic. We expect it to only be a matter of time before we see object detection optimized for video.
% Sum it up ?

%
\section{Limitations}\label{sec:conc_lims}
%
\section{Future Work}\label{sec:futurework}
%
% KIM: 60 kommentarer til denne section
Had we had our own video hosting service we would have access to such information.Had we further had our own client application in which all video was recorded we would even be able to acquire additional information about the circumstances in which the footage was recorded. Examples of this could be information about the movement of the camera (from accelerometers) and proximity to other devices recording the same scene (through Bluetooth). The purpose of our method for building our dataset has been to simulate that we do in fact have access to some of this information, as well as making it as easy as possible to estimate the rest.
%
image quality in a live system\\
more extreme alpha span value