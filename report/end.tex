\chapter{Discussion}
%
% labeling was so dependant on image quality that the quality assesment was not as much needed (other than optimizations we did not perform)
% impact of generally high quality videos (pre-edited and then cut up)
% gold standard is somewhat error prone
%
\chapter{Conclusions}
%
%Brief summary of the thesis
%
% chapter summary
%
% We have investigated different methods for estimating the image quality in video footage. Our approach is based on the work of Girgensohn et al.\cite{Girgensohn:2000:SAH:354401.354415} and focuses on the overall level of contrast in the individual frames as well as the movement of the camera.\\
% We have meassured the performance of four different classifiers on a dataset of videos collected from YouTube. We have not been able to confidently establish the importance of the contrast variable in the proposed method. We suspect this is caused by the fact that our dataset includes relatively few night-shots.\\
% Overall the results we achieve seems reasonable. They are however subject to a significant amount of risks caused by various choices and limitations.
%
In chapter \ref{chp:image_qual_ass} we assessed video image quality based on an approach by Girgensohn et al.\cite{Girgensohn:2000:SAH:354401.354415} where we analysed image contrast and identified camera movement in each frame of a video. Using four different classifiers we achived results compareable to an established gold standard.
%
% chapter summary
% We have investigated different methods for labeling video footage, depending on their content. We have made use of more advanced techniques, such as Haar Cascade classifiers\cite{viola01,lienhart01,schmidt01,schmidt02} (for detecting people), and Optical Flow\cite{Bouguet2000} (for analysing internal movement in the frames), but also experimented with simpler, home-made, techniques.\\
% We have created, and tested, classifiers for detecting crowds, overview footage, the presence of police blinkers, interesting persons and high levels of internal vertical movement. We have also created classifiers for labeling day-time and night-time footage.\\
% We experience a varying degrees of performance. \textit{Day/night}- and \textit{person in focus}- classification delivers promising results. Contrary, the performance of \textit{in-crowd}- and \textit{vertical oscillation}- clasification, is very bad. The two remaining classifiers, the \textit{overview}- and \textit{police blinker}- classifiers, deliver mediocre results.
%
In chapter \ref{chp:group_and_label} we developed and implemented methods for labeling video footage using already established work such as Haar Cascade classifiers and optical flow, and also some of our own design. Most of our label-classifiers achived respectable results, and were able to identify ex. public speakers with $\sim 90\%$ true positives.
%
% chapter summary (tmp.)
%
% We have described a way to automatically generate video summaries based on a defined \textit{recipe}. We have shown how, given a collection of labelled footage, we can estimate an overall fulfilment ratio for each segment of said footage, and use this an indicator of what to include in the final video.\\
% We have compared this method with various control groups, both consisting of randomly generated-, but also of humanly edited- videos. This was done through an online survey were a group of test users watched the different videos and rated them on various aspects.\\
% The results from this survey are inconclusive. The test users ability to differentiate between the various types of videos is not statistical significant. We have speculated on the various possible reasons for these findings.\\
%
In chapter \ref{chp:video_summaries} we designed a method of automating video summaries, and these summaries along with three different control sampes was subsequently evaluated by an online test panel. The results from the evaluation showed to be statistically insignificant, indicating that the panelists were unable to distinguish between the summaries and the control samples.
%
\section{Conclusions}\label{sec:conc_concs}
%
% Uncover the current state-of-the-art on the topic of automatic video summaries from multiple sources
Girgensohn et al.\cite{Girgensohn:2000:SAH:354401.354415} describes a method for a semi-automatic way of editing home video, in which they use automated video analysis to identify suitable clips. However they do not perform any contextual analysis of the footage. Our image quality assessment was based on methods presented by Girgensohn et al.\cite{Girgensohn:2000:SAH:354401.354415} who proposes a simple and effective way of detecting camera movement frame-by-frame by computing a vector describing the direction and magnutide of the movement. They also proposed a method of detecting overly dark images, on which we improved to also detect overly bright images, or images with little texture such as extreme closeups.\\
We proceeded with different approaches to classifying a single frame of video footage and compare the classification to a gold standard. Several of our approaches provided satisfactory results, but we do question the way we achived the gold standard as this included manual viewing of the entire dataset by one or more human observers and recording their subjective X. OMFORM!\\
% Develop and implement a method for labeling video segments.
We successfully labeled video footage based on a contextual analysis. The analysis was based on metadata from the image quality assesment, and metadata computed for the purpose of labelling. We were able to identify public speakers, footage shot from within a crowd, detect police blinker lights, and overview shots. 
\\
The correctness of each type of label varies, but the \emph{Person in Focus}-label was correct $X\%$ of the time. Other labels such as the \emph{Vertical Oscillation} bla bla. skriv mere om dette. SKAL BRUGE DISSE TAL!
\\\\
% Develop a method for composing a video summary from multiple sources with a coherent contextual flow
When we had completed labelling the video footage we moved on to creating the video summaries. We used a cooking analogy where a recipe is a list of ingredients, in a specific order, each defining a type of footage, that we would like to include in the final video. For each ingredient we try to find the most fitting sub-part of a video that matches the description of the ingredient, where the ingredient would be a list of parameters such as labels required and how long the part should be. For this purpose we designed an algorithm that computes a score for each sub-part based on an input ingredient and available footage.\\
We initially had plans of using machine learning for creating the summary, but this would require extensive training, possibly involving external feedback, and simply would not fit within the scope of the project.
\\\\
% Evaluate the final architecture and prototype with respect to domain-specific metrics (primarily focusing on quality and processing time)
The inspiration for this project based on an idea of a service that would process live video-streaming into a live video-summary. Such a service impose a low treshold on processing time for each video-stream. Image quality assessment is achiveable during playback and provides a good base for later optimization as we experienced that contextual analysis is mostly fruitless when performed on video footage of questionably quality. Ie. contextual analysis is only strictly needed on high quality footage, and an efficient approach as such. Object detection was one of the most ressource consuming processes, although capable of a live-analysis, we attempted to detect multiple objects (face, profile, and upper body), making multi-processing an absolute must. Most research the area of object detection in images is focused on still images, but research does exist on ex. detecting cars in traffic. We expect it to only be a matter of time before we see object detection optimized for video.
% Sum it up ?

%
\section{Limitations}\label{sec:conc_lims}
%
\section{Future Work}\label{sec:futurework}
%
% Had we had our own video hosting service we would have access to such information.Had we further had our own client application in which all video was recorded we would even be able to acquire additional information about the circumstances in which the footage was recorded. Examples of this could be information about the movement of the camera (from accelerometers) and proximity to other devices recording the same scene (through Bluetooth). The purpose of our method for building our dataset has been to simulate that we do in fact have access to some of this information, as well as making it as easy as possible to estimate the rest. -> BLARGH
% 
% more extreme alpha span value
% forbedringer i trin i pipeline?
%
% tager skidtet i kronologisk rækkefølge
%
% dataset
%
We achived respectable results when assessing image quality but as described in section \ref{sec:phase1discussion} our gold standard was greatly influenced by a dataset with primarily high quality footage. The way we created the gold standard was undoubtedly biased as it was done by the same person who would implement the classifiers and it would be interesting to see the results if the gold standard was subjected to a formal procedure including several different evaluations as the evaluation is subjective in nature.
\\
%
The results from our labeling classifiers was mixed, but we later realized that the poorly performing classifiers could be significantly improved by making some labels mutually exclusive, ie. using multiple weak classifiers we may be able to create several strong classifiers. The problem was remedied in a later phase of development and we do not expect it to have any impact on final results. We also skipped the time-consuming process of manually labeling the entire dataset to compute true and false negatives which would give us a more exact measure of performance of the label classifiers.\\
Besides improving upon already developed classifiers we could investigate the possibilities for new classifiers which would give us a larger number of labels, hence more diverse summaries. % nye labels
\\
%
As we were unable to measure any statistical significant effect in the different ways of creating a video summary it would be interesting to see if this would change if we solved some of the inherent falacies such as a dataset with far too little poor footage. The goal was to somewhat create a summary that was compareable to one created by a human which we achived depending on how one interprets the results, but we also did no better than randomly cutting footage together. We are also not sure if the test panelists fully understood the questionnaire, and a revised testing scenario coupled with a more rough dataset could provide a whole different outlook. % revise test
\\
%
The movitation and original idea included a service that would process live-streams and provide a near-live summary of these. We have shown that video-summaries is by all means an achiveable feat, and the next logical step towards such a service would included sorting which classifiers would be useable as is, and which would require improvement, either in classification-rates, or in efficiency. % live system