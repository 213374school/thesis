%
\section{Introduction}
%
News is big business, old news is not. We live in a world where we want the news as they happen, which mostly is not feasible. It takes a while for professional news crews to show up at a news-worthy event, but individuals present at the scene can sometimes become a decent substitute. The Internet contains literally millions of video clips from all around the world. This number is increasing rapidly, not just because more people have access to the Internet, but also because more and more people are equipped with video cameras, often in the form of smart-phones. It is therefore no surprise that often, when an event occurs, the first footage available consists of private recordings made by those present at the scene. With the increased access to high-resolution cameras and mobile Internet connections, we not only expect an increase in video uploads, but also in live-video streaming.\\
Currently, the vast majority of existing videos are published at video-streaming services such as YouTube, which, at the time this is written, receives 72 hours of videos footage every minute\footnote{http://www.youtube.com/t/press\_statistics}. The current possibilities for mining this enormous dataset are limited. It is possible to search for most videos using tags and keywords, but very little work has been put into grouping videos together around the events they cover. Given the expected increase in video uploads we argue that it will become increasingly important to begin looking at ways to cluster videos together and merge them into aggregated video streams covering entire events. By considering each video a scene located at a point in space and time, we theorize that it should be possible to produce single videos covering specific events, by merging neighboring scenes.\\
Extensive research already exists in the field of image processing and labeling and the Internet provides access to an abundant source of video footage. However, this footage is commonly of a rather low quality (at least compared to professional recordings). For example it can be too shaky or too dark.\\
With the processing power of distributed computing and the advancement in the field of image processing and machine learning in mind, we propose a system, which given a collection of video footage recorded at an event is capable of composing a watchable and semantically valid summary of that event shortly after the data becomes available. A system like this could, if it was fully developed, remove some of the human interaction in news curation and, as a result, decrease the delay between when 'the news' happen and when they become available.
%
\subsection{Overview}
%
In our work we focus on a selection of problems related to make such a video aggregator. We have divided this report into three main sections each covering one area of the process.
%
\subsubsection{Frame quality assessment} \label{sec:videoclipsegmentation}
%
Frame quality assessment refers to the process of identifying regions within the raw video footage where the overall image quality is reasonably good. Since footage may be very long or of poor quality it is important to be able to divide it into smaller sub-sections from which we can select the very best parts of the original video-clips. This can be used later on when the final video summaries are created, or simply as a way to remove unsuitable material before the more computionally heavy operations in the later phases. Even if one does not want to remove any material this early on, it could be used as a metric for prioritising later operations. This could be relevant in a live system where one might want to prioritise later analyses based on what footage is most likely to be most useful. Frame quality assessment is usually based on the lighting conditions in the individual frames as well as the overall camera movement throughout the video.
\subsubsection{Grouping and labeling}
%
Grouping and labeling refers 

We actually find that the later phase of grouping and labeling the footage, in our case, proved the initial frame quality assessment to be less important than we expected. In general, in order 
%
\subsubsection{Video summary aggregation}
%
%
\subsection{Limitations}
%
\subsubsection{Type of events}
%
The perfect system would be able to handle all kinds of different events. However, in order to limit the scope our work we have restricted ourselves to focusing on one kind of scenario: urban protests and demonstrations. Although this subject still yields diverse footage of everything from public speeches to police arrests it is only natural that some of the results we archive and the conclusions we make will not necessarily generalise to other kinds of scenarios.
%
\section{Data set}
\label{sec:dataset}
%
Our vision of a full blown system is one that constantly receives raw video footage of an event directly from the mobile devices recording it. This footage would then be processed by the system and used in a near-live coverage of that specific event. Since such a system currently does not exist, our goal is to build a data set which as closely as possible mimics this scenario. We do this by identifying a small collection of recent events, and then collecting usable video-clips of it from Youtube and other publicly available sources.
%
\subsection{Building the data set}
%
Since the vast majority of the videos available online do not suit our needs and since no efficient tool exist for efficiently finding the ones that does, the construction of our data set is a somewhat manual process. Each phase of this project poses specific challenges with regards to the data set needed for testing. Specific choices will be described in detail in their respective sections. However, similar for all, are the problems and decisions detailed below.
%
\subsubsection{Using videos from Youtube}
%
Our initial idea was to build the data set from Youtube videos recorded at events by selecting them based on location and time. However, it quickly became obvious that this was not feasible. The YouTube data API does not support searches based on specific time spans and even if it had, the timestamps accessible through it are based on the time of upload, not the time of recording. Since only very few videos are uploaded right after they are recorded, we found that the timestamps from videos from a specific event would be spread out over several days, sometimes even weeks. Location based search is however supported, but again we found the information accessible through the API unsatisfactory. Although videos can be tagged with location information, this data does not necessarily correspond to the GPS location of the device that recorded it, but is often generated based on an non specific place name, like a street, city or even country. And again, the information is created at the time of upload not the time of recording and thus based on the user's position at that time.\\
Given these observation we instead came up with another way to build our data set. When users upload a video to YouTube they can choose to attach a set of tags/keywords and categories to it. We found that these keywords and categories, when chosen correctly, can be a very efficient way to search for related video footage. For example, events like protests and demonstration can often be found by simply searching for a keyword describing the cause behind the event along with one describing the location (often a city). This way we are able to make a crude connection between the video footage and a specific time and location. Problems arise when the footage from a time and location can not be accurately identified through these attributes. For example, a video promoting a demonstration before it begins and explaining the reasons for it will often be tagged with the same keywords as the footage later recorded at the event. It may be possible to filter out such false-positives by further analysing its meta-data, but there does not seem to be a simple (or general) way to do this.
%
\subsubsection{Removal of unwanted material}
%
Since the purpose of our data set is to mimic raw, unedited footage as much as possible we have to perform a significant amount of manual filtering during our Youtube searches.\\\\
%
\textbf{Completely unrelated videos}\\
%
All videos that are obviously unrelated and only a result of ambiguous search queries are removed. Examples of this would be the police chase videos that emerged during our search for protest videos from the COP 15 Climate summit in Copenhagen in 2009.\\\\
%
\textbf{Related, non-event videos}\\
%
These videos are related to the event we want, but they are not from the event. Again, this is the result of a ambiguous search query or maybe even an unfit tag being applied to the video. Examples include videos that promote an event or, in the COP 15 example, videos regarding the climate meetings, which had nothing to do with the protests around it.
%
\subsubsection{Partly use of some material}
%
In some cases part of a video-clip may be suitable for a specific test although the clip in its entirety does not live up to our requirements. An example of this would be a video of an event that was obviously produced from several other video-clips. Although using such a clip as if it was raw footage would severely hurt the credibility of our results, some of its sub-segments (or maybe even the entire video) can sometimes be used in specific tests, for example as a “gold standard” of what good footage or a good video summary should look like.
% husk at det hedder golden standard (el. lign.) i stedet for ground truth
\subsubsection{Pre-processing of the videos}
%
The video-clips available online are recorded on a lot of different devices and stored in many different formats. In order to streamline the later analysis processes all videos are converted to the Apple MPEG-4 video format ‘m4v’. The conversion also resizes them to a dimension of 640x360 pixels at 1600 kbit data per second at 24 frames per second. These videos are of a size, which makes them manageable computationally when we later have to analyse them, while still preserving enough information about their content. % skal ikke nævnes her, men i den forbindelse skal vi også huske at nævne at vi analysere billedet i grayscale (selvfølge i billedanalyse, men hellere være på den sikre side). dette nævnes også nedenfor. måske vi skriver det i den omvendte rækkefølge?
%
\subsubsection{Noise in the data set}
%
%SOMETHING ABOUT THE POSSIBLE SOURCES OF NOISE IN OUR DATA SET
%
\subsubsection{Limitations as a result of our dataset}
%
A lot of the limitations in our research should be obvious at this point. We chose to only focus on a very small subset of all the possible scenarios a fully automatic system should be able to handle. Furthermore, we screen the data manually in order to make sure it is usable. However, much of the necessity for this is based on the fact that we do not have access to the raw footage as well as all the meta-data originally attached to a video-clip. Our main goal was to create a data set, which mostly contains original raw footage or, in the cases where it makes sense, video-clips, which we can pretend is raw footage. The task of determining whether a video-clip belongs to this category is, in many ways, much easier if you know exactly when and where it was recorded. Furthermore raw footage also often contain information about the source device it was recorded on, which again would tell us that the video data is unaltered. Had we had our own video hosting service we would have access to such information.
%